\documentclass[a4paper, 11pt]{article}
%\usepackage[round]{natbib}

\usepackage{mathtools}
\usepackage{setspace} 
\usepackage{dsfont}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{subcaption}
\usepackage{paralist}
%\usepackage{subfig}
\usepackage{times}
\usepackage{latexsym}
\usepackage{graphicx}
\usepackage[T1]{fontenc}
\usepackage{tikz}
\usepackage{url}
\usepackage{pgfplotstable}
\usepackage{titlesec}
\usepackage{color}
\usepackage{lipsum,adjustbox}
\usepackage[font={small}]{caption}
\usetikzlibrary{positioning}
\usepackage{bbm}

\makeatletter
\newcommand{\@BIBLABEL}{\@emptybiblabel}
\newcommand{\@emptybiblabel}[1]{}
%\makeatother
\usepackage[hidelinks]{hyperref}


\usepackage{naaclhlt2018}
\graphicspath{{./plots/}}
\newcommand{\com}[1]{}
%\newcommand{\oa\part{title}}[1]{}
%\newcommand{\lc}[1]{}
\newcommand{\oa}[1]{\footnote{\color{red}OA: #1}}
\newcommand{\oamod}[1]{{\color{red}#1}}
\newcommand{\lc}[1]{\footnote{\color{blue}LC: #1}}
\newcommand{\lcmod}[1]{{\color{blue}#1}}

\newenvironment{myequation}{
  \vspace{-1em}
 \begin{equation}
}{
 \end{equation}
 \vspace{-1.2em}
}
\newenvironment{myequation*}{
	\vspace{-1em}
	\begin{equation*}
}{
\end{equation*}
\vspace{-1.2em}
}


\begin{document}

\title{Reference-less Measure of Faithfulness for Grammatical Error Correction}
%\author{
%  Leshem Choshen\textsuperscript{1} and Omri Abend\textsuperscript{2} \\
%  \textsuperscript{1}School of Computer Science and Engineering,
%  \textsuperscript{2} Department of Cognitive Sciences \\
%  The Hebrew University of Jerusalem \\
%  \texttt{leshem.choshen@mail.huji.ac.il, oabend@cs.huji.ac.il}\\
%}
\maketitle

\begin{abstract}
  We propose {\sc USim}, a semantic measure for Grammatical Error Correction (GEC)
  that quantifies the semantic faithfulness of the output to the source,
  thereby complementing previous reference-less measures for measuring the output's grammaticality.
  The measure operates by comparing the semantic symbolic structure of the source and the correction (based
  on the UCCA semantic scheme \cite{abend2013universal}),
  without relying on manually-curated references.
  We conduct experiments to establish the validity of the measure,
  showing that UCCA can be consistently applied to ungrammatical text, that
  valid corrections indeed obtain a high USim similarity score to the source, and that
  poor corrections obtain a lower score. 
  %grammatical reference-less evaluation measure to
  %create a measure balancing between the goal of correcting grammatical mistakes and the goal of
  %conveying the original meaning.
\end{abstract}

\section{Introduction}

Evaluation in Monolingual Translation, and in Grammatical Error Correction (GEC) has been an active

\cite{tetreault2008native,madnani2011they,chodorow2012problems,bryant2015far}.
These difficulties, as well as the idiosyncracies of the GEC task has recently motivated
a number of proposals for new, improved reference-based GEC measures \cite{felice2015towards,napoles2015ground,dahlmeier2012better}\oa{there are also others, no?}, as well as datasets accompanied by more references.

Nevertheless, much due to the number and heterogenity of valid outputs

\footnote{Paper under review. Undisclosed to preserve anonymity.}, we show that due to the vast number of possible valid corrections, reference-based measures suffer from biases that will not be solved by more references. Specifically, systems developed towards such measures hardly correct, state-of-the-art systems are such systems. We thus propose {\sc USim}, a semantic reference-less evaluation measure which assesses the extent to which a correction faithfully represents the semantics of the source. {\sc USim} allows changes to the source but penalizes changes that affect the meaning. To complement this, a reference-less measure of grammaticality, based on automatic error detection as
proposed by \newcite{napoles-sakaguchi-tetreault:2016:EMNLP2016} should be used to incentivize systems to correct.

Semantic annotation schemes are also gaining support and proven to be useful for many tasks in general\lc{have preference on what to cite?} and specifically for evaluation \cite{birch2016hume}. One desired feature of semantic annotation is its robustness to modifications that do not change the meaning such as paraphrasing \cite{abend2013universal}, and translation \cite{sulem2015conceptual}.

Our experiments support the feasibility of the proposed approach,
by showing: (1) semantic structural annotation can be consistently and automatically applied to learner language, (2) {\sc USim} is not prone to unduly penalize valid corrections and (3) {\sc USim} does penalize corrections that alter the semantic structure significantly.



\section{The UCCA Sceme}\label{sec:ucca}

UCCA is a semantic annotation scheme that builds on
typological and cognitive linguistic theories.
The scheme's aims are to provide a coarse-grained, cross-linguistically
applicable representation.
Importantly, UCCA's categories directly reflect semantic, rather than
distributional distinctions.
For instance, UCCA is not sensitive to POS distinctions:
a Scene's main relation can be a verb but also an adjective
(``He is {\bf thin}'') or a noun (``John's {\bf decision}'').
Indeed, \newcite{sulem2015conceptual} have found that UCCA structures are
preserved remarkably well across English-French translations. 

UCCA structures are directed acyclic graphs, where the words in the text 
correspond to (a sub-set of) their leaves.
The nodes of the graphs, called {\it units}, are either leaves or several elements jointly
viewed as a single entity according to some semantic or cognitive consideration.
The edges bear one or more categories, indicating the role of 
the sub-unit in the relation that the parent represents.

UCCA views the text as a collection of {\it Scenes} and relations between them.
A Scene, the most basic notion of this layer, describes a movement, 
an action or a state which is persistent in time.
Every Scene contains one main relation, 
zero or more {\it Participants}, 
which are interpreted in a broad sense, 
and include locations, destinations and complement clauses,
and {\it Adverbials}, such as temporal descriptions.

\oa{please add a diagram}

\section{The {\sc USim} Faithfulness Measure}\label{sec:Semantics}
%
%
%Conservatism is considered an important trait for a corrector, reflected for example
%in the selection of $F_{0.5}$, which emphasizes precision over recall, as the
%standard evaluation measure in GEC.
%In the previous section we followed the common approach in GEC evaluation and evaluated
%
%The thought that stands behind such emphasis is that a user
%would be understanding towards errors he did, of which he is probably
%not even aware, not being corrected, but would not be so understanding
%of corrections altering what he meant to say, in a way he perceives as wrong.
%

The two major dimensions for the evaluation of GEC output is their grammaticaility
and their semantic faithfulness to the source.\oa{add reference}
{\sc USim} aims to capture the latter dimension 
through the graph similarity of their UCCA representations.

A faithfulness measure has to be complemented with an
error detection procedure \cite{napoles-sakaguchi-tetreault:2016:EMNLP2016},
as it only captures faithfulness, the extent to which the meaning of the source
is preserved in the correction and not its grammaticality. 

A similar decomposition of output quality to its adequacy (similar to faithfulness)
and fluency (related to grammaticality), has
been used in machine translation (MT) evaluation (e.g., \cite{banchs2015adequacy}).
Another related line of work studies MT reference-less measures
\cite{reeder2006measuring,albrecht2007regression,specia2009estimating,specia2010machine}.
Of which, perhaps the most closely related approach to ours is taken by MEANT \cite{lo2011meant}, which is based on a comparison of semantic role labeling structures on the source and target.

As a test case, we here use the UCCA scheme as a semantic representation \cite{abend2013universal},
motivated by its recent use in semantic MT evaluation \cite{birch2016hume}.
UCCA is motivated by its wider coverage of predicate types, as opposed to MEANT's focus on verbal structures. See \cite{birch2016hume} for discussion. Future work will experiment with a wider variety of semantic representations, both symbolic and distributional (e.g., \cite{cheng2015syntax}).


We conduct two experiments supporting the feasibility of our approach.
We show that semantic annotation can be consistently applied to learner language,
through inter annotator agreement (IAA) experiments and that a perfect corrector scores high on this measure.
We conclude by showing that the measure is sensitive to changes in meaning, by comparing
the semantic structures of the source to corrections of fairly poor quality.

%As semantic structures represent an abstraction over different realizations
%of a similar meaning,  In fact, $M$ plays no role in this approach, as the measure
%is defined not relative to a refernce but relative to the source sentence.
%
%as LL consists of many grammatical mistakes that makes syntactic
%analysis ill-defined for the task. We show evidence that this is the case, by having
%two annotators annotate a sub-corpus from the NUCLE dataset, and by measuring their
%inter-annotator agreement.
%%%Second, we ask whether corrections for a sentence indeed need to be faithful to the source. We seek to answer this question by measuring
%the semantic similarity between the source and the reference. We show support for an affirmative answer to this question 
%by annotating the references provided for the NUCLE dataset,
%and detecting high semantic similarity between the corresponding sentences on both sides. 
%
%\subsection{Background}
%
%Reliable assessment by a gold standard might be hard to obtain (see
%\ref{sec:increase-reference}), and human annotation for each output
%is great \cite{madnani2011they} but costly, especially considering the 
%development process. Under these conditions,
%%given a reliable semantic annotation we can enhance the reliability of our assessment. A simple way to do it is to somehow account in the assessment score for semantic changes. 
%Another, more ambitious way to do that might be to decouple the meaning
%from the structure. We propose a broad idea for a reduction from grammatical
%error detection and a comparable semantics annotation to grammatical
%error correction assessment. Lets assume we have both a reliable error
%detection tool and a good way to measure semantic changes. Then, we
%can transform assessment to a 3 steps assessment. 
%Step one, detect errors in the original text. Assess the amount of needed corrections, and the percentage of which that were changed.
%Step two, assess how much did the semantics change.
% Give a negative score for changing semantics.
%Last step, use
%the error detection again to assess how many errors exist in the correction
%output, whether uncorrected by the corrector or new errors presented
%by the correction process itself. 
%
%This assessment was partially inspired by the WAS evaluation scheme \cite{chodorow2012problems},
%in short it states we should account in the assessment for 5 types,
%not only the True\textbackslash{}False Positive\textbackslash{}Negative
%but also for the case where the annotation calls for a correction,
%and the corrector did a correction, but one that is unlike the annotation's
%one. With the proposed assessment we can measure how many of the corrections
%were corrected correctly (First + Second), and how many errors do
%we have eventually (Third) and combine them to get something similar
%to the Precision Recall that is widely used. We can also account for
%the places where the error was detected and check if it was corrected
%in a way that makes it grammatical and did not change semantics, the
%fifth type. We do that without getting a human to confirm this is
%indeed a correction.
%
%This system would be even more informative than the current one. Allowing assessment of
%what subtask exactly did the corrector fail to perform. Answering questions
%like: was the corrector too conservative and did not make enough corrections?
%Was it making changes in the right places but not correcting grammar successfully?
% as the corrector correcting grammar but changing things
%it was not supposed to? etc.
%
%Semantic structures were used for LL tasks \cite{king2013shallow}, but so far not to GEC. Some syntactic representations were suggested for this task, and as they are popular for structural representation we devote the next subsection to discuss them. Specifically we explain, why, apart from not being based on semantics, syntactic representation is not a good fit for LL and in particular not for enhancing the evaluation without enlarging reference number.
%
\subsection{Structural Representation in learner language}
%
%The usefulness of syntactic parsing in NLP has encouraged a number of previous
%projects to define syntactic annotation for learner language.
While linguistic theories propose that each learner makes consistent use of syntax \cite{huebner1985system,tarone1983variability}, this use may not conform to the syntax of the learned language, or of any other known language. This entails difficulties in defining syntactic annotation for learner language, as, on the face of it, the language of each learner has to be annotated in its own terms.

Learner language resources annotate syntactic errors in different ways.
\newcite{berzak2016universal} and \newcite{ragheb2012defining}
annotate according to the syntax used
by the learner, even if this use is not grammatical.
Such annotation may be unreliable as a source of semantic information, as by definition, systems aim to change those grammatical structures%semantically similar sentences, formulated by different learners, may use considerably different structures
.
\newcite{nagataphrase} take the opposite approach, and try to be faithful to the syntax intended by the learner, this is also the case in works on robustness of parsers assuming grammar should convey meaning and stay robust to errors \cite{bigert2005unsupervised,foster2004parsing}. However, such an approach faces difficulties due to the multitude of different syntactic structures that can be used to express a similar meaning. 

%
%Syntactic representation is very popular and useful in many NLP tasks 
%\cite{mesfar2007named,ng2002improving,zollmann2006syntax}.
%Thus, one thought that comes to mind is to use grammar annotation 
%for learner language.
%While not useless, grammatical approach is not well
%defined, and unclear both practically and theoretically.

In this section, we use semantic annotation to structurally
represent learner language text. Semantic structures are faithful to the intended
meaning of the sentence, and not to its formal realization, and thus face
fewer conflicts where the syntactic structure used diverges from
the one intended. We are not aware of any previous attempts to semantically
annotate learner language text.

\subsection{Experimental Setup}
We employ two annotators, and train them by annotating both learner language and standard English
passages, until a high enough agreement has been reached (a total of 6 hours of training).
Training passages are excluded from the evaluation.
We use UCCA's annotation guidelines\footnote{\url{http://www.cs.huji.ac.il/~oabend/ucca.html}} without any adaptations.

We experiment on 7 essays and their corrections, each comprising about 500 tokens.
In order to measure IAA, we assigned 4 of these essays to both annotators
and compute their agreement.
In order to measure the faithfulness score for a perfect corrector, we annotate both the source
and the corrected version for 6 essays, some of which were annotated by both annotators.

%
%For the different experiments 20 paragraphs were annotated, a table with the full
%information can be found in the appendix \ref{tab:annotated-paragraphs}.Overall, 2 learner language
%and 2 corrected paragraphs were annotated by both annotators, 9 parallel paragraphs were
%annotated by the same annotator and 6 by different annotators.
%
%We see that as enough to be a proof that UCCA can be applied to learner language, especially considering those numbers
%are a bit higher than the IAA originally reported
%for native English .
%We explain the rise in agreement by the fact that the guidelines and
%procedures were refined since UCCA was first introduced and not to
%superiority of UCCA for annotating learner language. A similar F1
%score for IAA (0.849) over corrected paragraphs
%suggests the same.
%
%At least theoretically, semantics are well defined even on ungrammatical
%text. With the right tools we might capture at least some of the semantics
%of sentences and use them for the same purposes of grammatical annotation.
%In this work we will use Universal Conceptual Cognitive
%Annotation (UCCA)\cite{abend2013universal}, we will show that practically
%there are semantic annotation schemes that can be used for the purposes discussed.
%
%
\subsection{Semantic Similarity Measures}
\paragraph{IAA Measure.} We define a similarity measure over UCCA annotations 
$G_1$ and $G_2$ over the same set of leaves (tokens) $W$.
For a node $v$ in either graph, define its yield $yield(v) \subseteq W$ as its
set of leaf descendants.
Define a pair of edges $(v_1,u_1) \in G_1$ and $(v_2,u_2) \in G_2$ to be matching
if $yield(u_1) = yield(u_2)$ and they have the same label.
Labeled precision and recall are defined by dividing the number of matching edges
in $G_1$ and $G_2$ by $|E_1|$ and $|E_2|$, respectively, and
the {\it DAG $F$-score} is their harmonic mean.
We note that the measure collapses to the common parsing $F$-score if $G_1$ and $G_2$ are trees.

\paragraph{Semantic Faithfulness Measure.} Computing a faithfulness
measure is slightly more involved, as the source sentence graph $G_s$ and its
correction $G_c$ do not share the same set of leaves.

%Giving a more accurate
%measure than the upper bound suggested by \cite{sulem2015conceptual} for
%comparing two parallel texts in different languages, while keeping
%the essence of comparing how many of the aligned nodes conserve meaning and tag. For that we may think for a moment on GEC as
%translation from learner language to Proper English, and a good translation
%would be a translation which keeps the meaning but has the syntax
%of English. Considering that, just like in translation, we can align words from the learner language to the corresponding words in English
%and keep record of how many of those nodes kept their labels.
%
%As comparing labels is trivial, and done before us. We should focus on how we propose to align nodes. 
%First, note that comparison should not be at
%the token level, as we want to allow tokens to be corrected - replaced or removed -
%as long as the higher structures convey the same meaning. We thus
%prune the labels above the leaves, the tokens of the sentence. To
%define an alignment of the nodes, we suggest some possible ways, all
%based on first aligning the words in order to give order to the DAG and then comparing the structure in one way or another.
%
We assume a (possibly partial, possibly many-to-1) alignment between $G_s$ and $G_c$,
$A \subset V_s \times V_c$. An edge $(v_1,v_2) \in E_c$ is said to match an edge
$(u_1,u_2) \in E_s$ if they have the same label and $(v_2,u_2) \in A$. Recall (Precision)
is defined as the ratio of edges in $E_s$ ($E_c$) that have a match in $E_c$ ($E_s$) respectively, and
$F$-score is their harmonic mean. We note that this measure collapses to the
{\it DAG $F$-score} if $A$ includes all pairs of nodes in $E_s$ and $E_c$ that have
the same yield.

In order to define the alignment between $V_s$ and $V_c$, we begin by aligning the leaves
(tokens) in $V_s$ and $V_c$.
We cast alignment as a weighted bipartite graph matching problem. Edge weights are assigned to be the edit distances between the tokens.
We note that aligning words in GEC (and other monolingual translation tasks) is much simpler than in machine translation,
as most of the words are unchanged, deleted fully, added, or changed slightly.
Denote the resulting leaf alignment with $A_l \subset Leaves_s \times Leaves_c$.
We now extend $A_l$ to define the node alignment $A$, aligning each non-leaf $v \in V_s$
with the node $u \in V_c$ that maximizes

\begin{small}
	\vspace{-0.1cm}
 \begin{myequation*}
  w\left(v,u\right) = \frac{\vert A_l \cap \left(yield\left(u\right) \times yield\left(v\right)\right)\vert}{\vert yield\left(u\right) \vert}.
 \end{myequation*}
 \vspace{-0.1cm}
\end{small}

We exclude from $A$ pairs $v,u$ such that $w(v,u)=0$.
The resulting $F$-score measure, using the resulting $A$ is called UCCA Similarity ({\sc USim}).
As the resulting alignment may differ when aligning nodes from $V_c$ to $V_s$
and the other way around, we report the resulting $F$-score in both directions.

Note that {\sc USim} is somewhat more relaxed than {\it DAG $F$-score},
as it also aligns nodes whose yields are not in perfect alignment with one another,
unlike DAG $F$-score which requires a perfect match.
While this relaxation is necessary, given that corrections often add
or remove nodes, thus eliminating the possibility of a perfect alignment,
in order to obtain comparable IAA scores, we report IAA using {\sc USim} as well.
%
%Then for each node in $v \in V_s$, we compute its descendent leaves $yield(v) \subset Leaves_s$ as before,
%and compute their projection $yield'(v) = \{u \in Leaves_c:(v,u) \in A_l\}$.
%We now define the node alignment to be $A = \{(u,v) \in V_s \times V_c : yield'(u) = yield(v) \}$.
%We note that $A_l \subset A$ and that if $V_s$ and $V_c$ share the same tokens,
%this computation again reduces to DAG $F$-score.

For completeness, we replicate the protocol used by \newcite{sulem2015conceptual}
for comparing the UCCA annotations of  standard English-French translations, which we call
Distributional Similarity ({\sc DistSim}).
For a given UCCA label $l$, $c_i(l)$ is the number of $l$-labeled UCCA edges
in the i-th source sentence, and $d_i(l)$ is the number of $l$-labeled UCCA edges
in its corresponding correction. We define {\sc DistSim}(l) between these
sentences to be $\frac{1}{N}\sum_{i=1}^N \vert c_i(l) - d_i(l) \vert$, where
$N$ is the total number of sentence pairs.
%
%A first and most straightforward approach would be to compare all
%pairs of nodes in parallel paragraphs and to each node from one paragraph
%assign the one most similar node, span wise from the other. That approach
%is quite similar to the IAA aligning, but it
%has three drawbacks; it is asymmetric; it may be over optimistic aligning
%nodes without considering the DAG structure; and it might be
%slow for many nodes. Being asymmetric is not much of a problem. If we thrive for symmetry
%we can compute the measure twice and use the results mean,
%that would also be the case for other asymmetric methods we suggest.
%In order to address the other drawbacks we propose different aligning methods.
%
%A second method driven by the assumption that nodes higher in the
%hierarchy are more important to the semantic representation is measuring
%the largest cut in which nodes are aligned (top down) to each other
%and have the same labels. This is a harsher lower similarity
%score but one of which might be more representative of the semantics
%that are kept and hopefully more informative for tasks that will use it.
%
%A third type of methods were token similarity methods, these methods
%use one kind of aligning (top down, bottom up or pairwise) and only
%compare the meaningful nodes. This was called upon by \cite{sulem2015conceptual}. 
%This approach makes sense due to the fact that some labels
%are well defined and thought upon while others are still vague and
%call for future work on refining or adjusting them, moreover, some
%labels are more semantic while other labels are currently just a place
%holder as each node must get a level, and the semantic role is not
%always clearly defined (e.g., the word ``is'' in ``he is walking''
%seem to be more syntactically related than semantically. In UCCA it is tagged as a \textit{function} word.). The unused
%labels are center, elaborator, function, relation, linker, ground
%and connector.
%
%A bit different way than all the others is to compute the labeled
%tree edit distance\cite{zhang1989simple}, for that we first needed
%the trees to be ordered, we did that in a top down fashion. An interesting
%future work would be to use unordered tree edit distance methods\cite{zhang1992editing}.
%
%We see that measurements for symmetry that are similar to the inter
%annotator agreement measure also suggest high stability, achieving
%scores not much lower than the one different annotators get for the
%same paragraph. This result is quite strong as an inter annotator
%agreement is the upper bound being the score of comparing a paragraph to itself. 
%Most importantly we learn from it all that even when correcting grammatical errors,
%the semantic structure (as represented by UCCA at least) is hardly changed and thus
%can be used as a tool to avoid introducing semantic changes when trying to only change grammar. 
%The symmetry measures we introduce can be used to enforce semantic conservatism.
%In conclusion, we have shown some direct ways to measure
%semantic faithfulness. Such ways will allow correctors to be less formally conservative
%while controlling semantic conservatism. Thus, focusing on what users - and hence we - are more interested in.
%
%
\subsection{The Faithfulness of a Perfect Corrector}
We obtain an IAA {\it DAG $F$-score} of 0.845
(Precision 0.834, Recall 0.857), which
is comparable to the IAA reported for English Wikipedia texts by \cite{abend2013universal}.As another point of comparison, we doubly annotate 3 corrected
NUCLE \cite{dahlmeier2013building} passages, obtaining a similar IAA.
These results suggest that annotating learner language with UCCA does not degrade IAA, and can be applied as consistently to learner language as to standard English.

Table \ref{tab:Distances} (left)
presents the {\sc USim} scores obtained by comparing the NUCLE references and the source, or equivalently the score of a perfect corrector.
To control for differences between the annotators, we explore both
a setting where both sides are annotated by the same annotator,
and a setting where they are annotated by different ones.
As an upper bound on the score of a perfect corrector (using different annotators),
we also report the {\sc USim} IAA on source sentences. 

Our results indicate that a perfect corrector obtains a score comparable
to the IAA, which indicates that {\sc USim} is indeed
insensitive to the surface divergence between a source sentence and its valid corrections.

%
%Finally, we present as a control measure and a bound on the best score
%we can expect to get in such comparison the scores of paragraphs
%in which we compare two annotations for the same paragraph using all
%the similarity measures discussed, it can be thought of as a different
%way to defining IAA. Note that a similarity
%of 1 and distance of 0 is indeed reached when comparing an annotation with itself.

Finally, the right-hand side of Table \ref{tab:Distances} presents {\sc DistSim}
between the source and reference sentences.
Our results are similar to the ones obtained by \newcite{sulem2015conceptual},
mentioned before.
\begin{table}
	\vspace{-0.5cm}
  \small
  \centering
  \singlespacing
  \begin{tabular}{c|c|c|c||c|c|}
  	\cline{2-6} 
  	& \multicolumn{3}{c||}{\sc USim} & \multicolumn{2}{c|}{\sc DistSim}\\ \cline{2-6}
  	& s$\rightarrow$r & r$\rightarrow$s & Avg & A+D & Scene\
    \\
    \hline
    Different & 0.85 & 0.83 & 0.84 & 0.96 & 0.93
    \\
    Same & 0.92 & 0.91 & 0.92 & 0.97 & 0.96
    \\
    \hline
    \hline
    IAA & 0.85 & 0.81 & 0.83 & - & -
    \\
    \hline
    SAR15 & - & - & - & 0.95 & 0.96 \\
    \hline
  \end{tabular}
  \caption{\label{tab:Distances}
    The faithfulness of a perfect corrector. The left-hand side presents {\sc USim}
    where the alignment is computed from the source to the reference (s$\rightarrow$r), the opposite direction
    (r$\rightarrow$s), and their average (Avg).
    %The direction of the alignment evidentally has little impact on the results.
    The right-hand side presents {\sc DistSim} for the UCCA categories Participants and Adverbials
    together (A+D), and Scene (Scene), as reported by \newcite{sulem2015conceptual}.
    The rows indicate whether the same annotator annotated the source and reference or not. As an upper bound, we report IAA computed using {\sc USim} (IAA row).
    Results show that the perfect corrector's faithfulness is comparable with IAA.
    The bottom row presents the results reported by Sulem et al. (SAR15) on English-French
    translations, which are comparable to ours.}
\vspace{-0.6cm}
\end{table}

\subsection{Automatic USim}

%After showing corrections are indeed similar to the source semantically, more or less in agreement like IAA, one would justifiably wonder if this can be captured in an automatic way.
We turn to experimenting with an automatic variant of USim, where the UCCA
structures are parsed automatically.
We use TUPA \cite{hershcovich2017transition} to generate the UCCA structures,
instead of the human annotators, replicating all other details of the experimental setup. 
TUPA is used in its biLSTM model, trained on the UCCA English Wikipedia corpus.

We obtain a USim of 0.7 between the automatically generated parses of the reference
correction and the learner language source. This similarity is comparable to the parser's reported
performance (0.73 in-domain, 0.68 out-of-domain), despite not performing any
domain adaptation to learner language. 
That is, the UCCA parses of the source and correction are roughly as similar to each
other, as they are to their gold standard parse, which indicates 
that semantic parsing technology is already sufficiently mature to
apply to automatic faithfulness measures, such as USim.
Results also suggest an improvement in parsing performance may further improve these scores.

%We note that the parser was not trained again in order to capture learner language. Still, the amount of agreement is more or less the score the parser gets.

\com{
\begin{table}
	\centering
	\singlespacing
	\begin{tabular}{c|c|c|c|}
		\cline{2-4} 
		& \multicolumn{3}{c|}{\sc USim} \\
		\cline{2-4}
		& s$\rightarrow$r & r$\rightarrow$s & Avg\
		\\
		\hline
		TUPA & 0.7 & 0.7 & 0.7
		\\
		\hline
		\hline
		Different & 0.85 & 0.83 & 0.84
		\\
		\hline
	\end{tabular}
	\caption{\label{tab:parser} The table presents {\sc USim}
	  where the alignment is computed from the source to the reference (s$\rightarrow$r),
          the opposite direction (r$\rightarrow$s), and their average (Avg).
	  The first row presents results using TUPA parser \cite{hershcovich2017transition}.
          The second row we see the results of one annotator for the source and another for the reference.
	  The results show that the perfect corrector's faithfulness is captured quite
          well with the automatic parsing, around the parser reported accuracy and standard English.}
\end{table}
}

\subsection{USim's Sensitivity to Errors}

Our results have hitherto shown that UCCA is insensitive to differences between a source sentence
and its valid correction. This sub-section presents an evaluation of the sensitivity of USim
to proposed corrections that diverge semantically from the source.

%To wrap up the argument we show that {\sc USim} is not only insensitive to corrections (Table \ref{tab:Distances}) but also sensitive to meaning changing corrections.

From a theoretical standpoint, a semantic measure is, by its definition, sensitive to variation in
the semantic dimensions which it encodes. 
In UCCA's case, these distinctions focus on predicate-argument structures, the inter-relations between them,
and the semantic heads of complex arguments. These distinctions are widely accepted in NLP and
in the linguistic literature, as fundamental.

In order to empirically validate this claim, we present an experiment which shows that corrections
of a fairly low quality indeed receive a much lower USim faithfulness score. As the state-of-the-art systems are over-conservative\footnote{Paper under review. Undisclosed to preserve anonymity.}, they produce few semantically unfaithful sentences, and hence are not useful to our cause\footnote{For the same reason available human judgments do not provide meaningful information about USim's ranking, but only about the grammatical measure used with it.}. We 
instead experiment on 5 partially trained correctors, trained and evaluated on the
JFLEG corpus \cite{napoles2017jfleg} by \newcite{sakaguchi2017grammatical}.

{\sc USim} was computed automatically for each system's output on 754 source sentences.
Low faithfulness results are expected, as these outputs include major changes,
sometimes deleting full phrases from the output or changing every other word in the sentence.
Indeed, the automatic USim measure obtained scores of 0.32-0.39 for 4 of the systems, and 0.19
for the system, which obtained the worst GLUE score.
For completeness, we ran {\sc USim} on the 4 references provided for each
source by JFLEG and obtained scores of 0.72-0.78.

Taken together, these results indicate that {\sc USim}, even in its automatic variant,
is sensitive to semantic changes. Consider the following example: 

\begin{table}[h!]
	\vspace{-.5cm}
  \centering
  \label{ex:sensitive}
  \begin{tabular}{p{0.2\columnwidth}p{0.73\columnwidth}}
    Source    & \small the good student must know how to understand and work hard to get the iede.\\
    Reference & \small A good student must be able to understand and work hard to get the idea.\\
    Corrector & \small The good student must know how to understand and work hard to get on.     
  \end{tabular}
  
  \vspace{-.5cm}
\end{table}

{\sc USim} assigns the reference 0.71 and only 0.33 to the corrector.
Moreover, although the reference makes more word changes than the proposed correction,
it still obtains a higher USim score.
%, as (unlike the proposed correction), it retains the
%predicate-argument structure.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%was shown to have some wanted attributes that the $M^2$ scorer lacks.
%For example, it scales from -1 to 1 providing a way to know if a correction is an improvement over the
%source. I-measure expects the same input as $M^2$ but its score differs as it is based on token-level
%edits accuracy score. 
%
%Addressing the need to improve automatic GEC evaluation, three sophisticated measures have been proposed,
%all of them are reference based. 
%$M^2$ was introduced for CoNLL2013, providing a way to compute phrase-level edits $F$-Score. As an
%input $M^2$ expects a source sentence, a correction and a set of edits for each reference in the
%gold standard.
%It uses an edit lattice to optimistically choose edits for the reference that
%will best match those of the references. Since it was introduced $M^2$ is the standard scorer for GEC.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion}

This paper proposes a measure for the semantic faithfulness of the correction to the source,
thereby avoiding the pitfalls of reference-based evaluation. We believe that using reference-less
measures in conjunction with reference-based measures in the training and development of GEC
systems will better address the challenge of over-conservatism, and the high costs of acquiring many references.

Future work will assess the relative importance, ascribed by users of GEC systems,
to different evaluation criteria of the output.
Specifically, we will explore to what extent users are
tolerant to changes in the sentence structure, i.e.,
violation of conservatism, relative to their tolerance to changes in the sentence's meaning,
i.e., violation of faithfulness.
A better understanding of how these interact
may lead to improved semantic evaluation, that will alleviate the need
for a high number of references.


\bibliographystyle{acl_natbib}
\bibliography{propose}


\com{
\appendix
\section{Annotated paragraphs}
\begin{table}[]
	\centering
	\begin{tabular}{lll}
		Annotator-id & NUCLE-id & type      \\
		1         & 2  & corrected \\
		2         & 2  & corrected \\
		1         & 2  & learner   \\
		2         & 2  & learner   \\
		1         & 3  & corrected \\
		2         & 3  & corrected \\
		1         & 3  & learner   \\
		2         & 3  & learner   \\
		1         & 5  & corrected \\
		2         & 5  & corrected \\
		1         & 5  & learner   \\
		2         & 5  & learner   \\
		1         & 6  & learner   \\
		2         & 6  & learner   \\
		2         & 7  & corrected \\
		2         & 7  & learner   \\
		1         & 8  & corrected \\
		1         & 8  & learner   \\
		1         & 10 & corrected \\
		1         & 10 & learner  
	\end{tabular}
	\caption{The list of paragraphs annotated, showing which annotator annotated it, which type of language is used in it and the corresponding id in the NUCLE corpus. Note that parallel paragraphs have the same id.\label{tab:annotated-paragraphs}}
\end{table}
}
\end{document}
