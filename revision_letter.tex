\documentclass[11pt,letterpaper]{letter}
\usepackage{fullpage}
\usepackage{url}
\usepackage{color}

%\addtolength{\topmargin}{-.5in}
%\addtolength{\textheight}{1.5in} %for home
\newcommand{\todo}[1]{{\bf [#1]}}
%\newcommand{\oa}[1]{}
%\newcommand{\lc}[1]{}
\newcommand{\oa}[1]{\footnote{\color{red}OA: #1}}
\newcommand{\oamod}[1]{{\color{red}#1}}
\newcommand{\lc}[1]{\footnote{\color{blue}LC: #1}}


\signature{Leshem Choshen}
\address{
The Department of Computer Science \& \\
The Department of Cognitive Science \\
The Hebrew University of Jerusalem \\
Rothberg Building A, Room A421 \\
Edmond Safra Campus, Givat Ram \\ 
Jerusalem, 91904, Israel \\
Phone: +972-545961029 \\
email: leshem.choshen@mail.huji.ac.il
}

\begin{document}

\begin{letter}{%\vspace{-.3in}\\
Hwee Tou Ng\\
Action Editor\\
{\em TACL}\\
%\vspace{-.15in}
}

%\setlength{\parskip}{0in}
%\setlength{\parindent}{.5in}


\vspace{.5cm}

  \opening{Dear Prof. Hwee Tou Ng,}


\vspace{.5cm}
  
  Please find attached the revised version of our manuscript ``Conservatism and Over-conservatism in Grammatical Error Correction''. We thank you and the reviewers for your helpful and thorough comments, and have made significant changes to the presentation in response. In particular we addressed the following issues: conservatism is empirically caused by low coverage, number of state-of-the-art correctors checked for conservatism, quality of low frequency corrections, all things related to UCCASim can be done automatically with current parsers and the sensitivity of UCCASim.

\vspace{.5cm}

{\large\bf Stronger Evidence for the validity of UCCASim (Section 4)}

\oa{which reviewer said what? put next to each R1, R2, R3} \footnote{ok, i'll do it}
\oa{we need to mention the additional exp in section 2 somewhere}\footnote{don't we?}

\emph{``... why it follows that a
	particular 'reference-less' semantic measure (UCCASim) is a solution to the
	identified issues introduced by the RBMs. The authors suggest that UCCASim
	is insensitive to surface changes between the source and NUCLE (gold)
	reference sentences and that inter-annotator agreement is reasonably high.
	However, importantly, the authors have not shown whether the measure is
	sensitive to semantic errors, so it is not clear whether the measure
	provides a useful signal. (Interestingly, the authors seem to suggest that
	such semantic errors very rarely occur in the data, preventing such
	experimentation directly.)''
}

The paper now includes an analysis, using UCCASim, of several systems that do tend to
make major (and often erroneous) changes to the source. UCCASim indeed deems them to be semantically
unfaithfulness to the source, indicating that the measure is sensitive to semantic errors. See Section 4.6.

\emph{``Where is the evidence that the new semantic adequacy metric \_actually\_
	solves the problem of correctors being over-conservative? The analysis the
	authors contributes nothing towards that. To determine this, one needs to
	actually compare systems tuned using this new metric on the same NUCLE
	dataset vs. a system tuned using the existing F0.5 metric and see whether
	they have different correction behaviors.''
	}

This issue is now addressed in Sections 4.4 and onwards.
We would argue that the important experiment is the one we added in section 4.6, because it shows that not only is UCCASim insensitive to meaning-preserving variation, but there is also indication that it is sensitive to meaning-changing variation. This shows the merit of UCCASim as a semantic faithfulness measure, which is what we argue for.
Tuning a system by a semantic faithfulness measure, is, if at all, likely to increase conservatism, as the source sentence is maximally faithful as a proposed correction. It is the combination of this measure with a grammaticality measure that, we argue, would decrease conservatism, but it is the grammaticality measure that pushes towards less conservatism, where the faithfulness measure modulates this effect, so not to change the semantics. In light of this, we believe that interpreting the results of the experiment the reviewer proposes would be difficult, and instead provide other evidence for the validity of the approach.

%, as well as an additional also added an experiment showing that additional references decrease conservatism in section 2

\vspace{.5cm}
{\large\bf UCCASim vs. other measures:}

\emph{``Why propose a completely novel metric in Section 4 that requires human
	annotation? Why not use an existing semantic similarity methods? There are
	literally dozens of choices when it comes to measuring semantic similarity.''}

	\emph{``The authors are not proposing (or
	testing) an automatic means of parsing UCCA. As such, since human annotation
	is necessary, why not have the humans directly assess semantic faithfulness?''}

\emph{``Even if you are using UCCA, why come up with an entirely new metric? Why not just use HUME which Birch et al. use for MT?''}


\emph {``Is the UCCA formalism a better choice than semantic role labeling, and, if so, why?''}

We by no means claim that UCCASim is the only appropriate measure for semantic evaluation, and have now stressed this in the paper, as well as added some discussion in the beginning of Section 4. We selected UCCA and not other semantic methods due to some of its appealing properties as a basis for semantic evaluation, as discussed in ``HUME: Human UCCA-Based Evaluation of Machine Translation'' (Birch et al., EMNLP 2016), primarily that UCCA supports a wide variety of semantic structures, including copula clauses, nominal and adjectival structures, as well dicourse markers.

In addition, UCCASim can be automated. Indeed, the revised submission includes results using an automatic variant of UCCASim. This is also the reason for not selecting HUME, which is not fully automatically.


\vspace{.5cm}
{\large\bf Comments on the Data:}

\emph{``Using 7 essays for an annotation study seems ridiculously low.''}

The annotation study includes 13 annotated essays, 10 doubly-annotated essays (7 source essays, 3 corrected essays) each of about 500 words. These results serve to show that the IAA scores obtained for UCCA (reported, e.g., in Abend and Rappoport / ACL 2013), also hold for the domain in question (LL and corrected LL). As the results match previously obtained results, they serve as a confirmation, and thus seem adequate in size. The results obtained with the UCCA parser TUPA, provide further support for our results.

\emph{``The authors do not mention exactly which version of the NUCLE corpus
	they used in Section 2. If it is the 2014 CoNLL data version, that test data
	has corrections from multiple annotators. Did you account for the multiple
	annotators? How is the gold row computed for the Word-Change heat map? There
	are multiple references so is that the average? Given multiple references,
	isn't it natural to expect the average to behave the way that it does and
	not just because the correctors are conservative?''}

We used the 2014 test set, selecting arbitrarily one of the references for all the analysis, avoiding the above mentioned problems. We have now also clarified this in the text. Please note that a similar effect was obtained on the FCE corpus.

\vspace{.5cm}
{\large\bf Other Related Comments:}
	
\emph{``Very little is said about the DistSim results in Table 2 - I wondered why
	they were even included.''}
	
Indeed these results are only presentated for the sake of comparability with the English-French results of Sulem et al.
	
\emph{``Some old works on robustness evaluation are somewhat relevant here since
	they propose that the structure of source and corrected sentences be
	compared in order to check that meaning has been preserved.''}
	
Thank you for these references. We have now added them to the text.
	
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
{\large\bf Conservatism and Comments realated to Section 2}

\emph{``Other recent published systems on the CoNLL 2014 data, e.g., Junczys-Dowmunt
	and Roman Grundkiewicz 2016, are not included. This is notable, since some
	of the more recent systems are significantly higher performing than those of
	the original shared task 3 years ago.''}
	
We included an evaluation of the conservatism of Junczys-Dowmunt and Grundkiewicz's corrector  and another character-based neural network (Xie et al.) -- they are not substantially different from the others.

\emph{``It would be interesting to see the differences between the over-conservative
	behaviour of the systems when $f_1$ is used as the evaluation metric and when
	$f_{0.5}$ is used.''}

About $f_1$: although Rozovskaya et al. report it, it has rarely been used since 2014 and hence systems developed with $f_1$ in mind are old and not comparable.
	
\emph{```We believe that in terms of conservatism, end users will be tolerant to
	changes in the sentence structure, i.e., violation of conservatism' Wouldn't
	this very much depend on the application?''
}

\emph{``The assumption that users would tolerate
	some mistakes just to have the system be able to suggest more corrections
	requires further justification to me.''
}
	
The point is taken that it may be desirable for correctors in some contexts to be more conservative than human correctors. We changed our text to reflect this. However, even if this is the case, the effect we find is relevant because (1) there is an order of magnitude difference between humans and correctors; (2) the conservatism is such that current systems already surpass the F-score obtained by humans, which may suggest that other measures should be used from now on to evaluate GEC systems. {\color{red}(3) over-conservatism can be seen not as a user related issue but as a research measure. In this view, the goal is to eventually be able to be able to have a perfect system, a strong bias to such as this over-conservatism would be an indication that something might be wrong. In our case it seems that indeed it is not only that correctors can not possibly correct more than they do, but that it is probable the methods used incentivize them not to.}
%(3) over-conservatism can be thought of as places were correctors can achieve corrections with high enough precision but they are over-conservative and avoid correcting even when unnecessary. Then the graphs we show are indicating it might be the problem, but not proving it (nothing actually could, as this is counter-factual). 

%More speculatively, even if over-conservatism is desirable, low coverage of references is still an issue, as it likely biases over-conservatism much more on these cases where there are many possible valid corrections, presumably more complicated errors. In this sense, over-conservatism may be an indication of issues with the training protocol.

\emph{``The examined systems may well under-predict errors (low recall); however,
	incorrect predictions (precision) are also (likely) a problem for many error
	types for many of the systems. It would be useful to see an analysis or
	motivation as to whether or not low recall is the primary effectiveness
	bottleneck in practice. It could be that in practice, the systems are
	'under-conservative' if end-users prefer high-precision results.''
	}

We agree it would be very helpful to supplement this research with a user study of GEC systems, in order to locate potential bottlenecks. However, we would argue that the current evaluation is problematic as it pushes during development not to correct, even in ``high-confidence'' decisions, where precision may have been high, just because the coverage is low. 

\emph{``Wouldn't it also be instructive to actually look at the distribution
	over the exact error types for the analyses in Section 2? Perhaps the automated correctors are heavily conservative for only one or two types of errors?''
	}
	
We agree it would be instructive, and we indeed plan to pursue this in future work. Note that there are several methodological issues to be resolved to implement this, such as defining what constitutes a type where the annotators disagree with one another, and how to categorize the correctors' edits, {\color{red}which come without types and include negative edits as well, which might be correct or not.}
	
\emph{Why use two separate word-level measures in Section 2? Why not just use
	an MT metric like METEOR/TER that uses edit distance and takes into account
	word ordering?}

It seems to us that METEOR less transparently reveals what type of changes are made and not made to the source. Nevertheless, we experimented with METEOR and obtained similar results, namely that no corrector obtained mean METEOR score that is lower than the references' (when compared to the source).

{\bf Corrections as a distribution}

\emph{``The analysis in Section 3 is definitely somewhat interesting and
	well-done (bringing in estimation procedures from a different field) but not
	too novel. Bryant and Ng (2015) already showed that there are problems of
	underscoring with the F0.5 metric and that not even humans score highly on
	that, as the authors acknowledge.''
	}
	
We agree that the experiments in section 3 are similar to the one's reported by Bryant and Ng (2015). Still, our work presents a number of contributions: (1) we evaluate not only the coverage of the measures, but also the distribution of corrections, showing that even increasing M considerably is unlikely to much improve the under-estimation; (2) we discuss the relation of these results to over-conservatism, (3) we show that due to the observed over-conservatism, GEC systems may surpass human performance in terms of F-score, suggesting that non-RBM may be needed.
%and (4) due to (2) and (3) we suggest a different solution as we see a different problem that was out of Bryant and Ng (2015) scope.

\emph{``Motivating Analysis: Note that the connection to training is more
	complicated than implied here, at least with the evaluated systems. Namely,
	most of the reviewed systems are actually not trained/tuned, at least
	directly, against the final RBM, which was $M^2$ for most of the systems here.''
	}
	
	We agree, although during development, systems are tuned against an RBM. It is meant to use to exemplify how over-conservatism may be linked to low coverage. We also mentioned in the text that "we abstract away" the details. We have now also included an empirical analysis to this effect. One of the reasons to base it on reranking is that development or validation either automatic or by hand is much more likely to use RBMs.\footnote{isn't it here already? "We have now also included an empirical analysis to this effect."}\oa{mention the new exp in section ``motivating analysis''}
	
	\emph{``It would be useful to include additional sensitivity analysis of the
		UnseenEst algorithm to get a sense of the quality of the distribution
		prediction. For example, you could have a large number of AMT workers
		correct a small number of sentences, providing additional annotations
		(beyond the original 50) and compare the resulting distributions to the
		estimated distributions. On low probability corrections, it would also be
		useful to assess inter-annotator agreement: Namely, it could be that
		low-frequency corrections have low inter-annotator agreement, in which case
		it may be preferable (in practice) for correctors to avoid making
		predictions in the tails of the distribution.''
		}
		
Zou et al. (and the manual distributions we created and checked) show UnseenEst behaves well (in the paper there are strong guarantees on that). More specifically, Zou et al. show with different experiments its ability to predict the number of variants and the frequencies when seeing only 10\% of the variants. 
%Assessing UnseenEst using AMT created distribution would actually just be to create another distribution at a high cost, as the algorithm is non-parametric and at the end removes the ``labels'' (corrections themselves) and only looks at the histogram.
Thank you for the note about the reliability of the less frequent corrections. We therefore conducted an additional experiment (results in Section 3.3 and Figure 3), in which we demonstrate that there is no major difference between the validity human annotators assigned to the validity of the frequent corrections, as opposed to the infrequent ones.
%We sent all of our corrections to AMT and they told us whether they think those are corrections or not. Using the empirical frequency of the corrections we reported the results in this version of the paper. In short, rare corrections are good as well as frequent ones.
		
\emph{``More detail should be provided on how UnseenEst works.''}
		
More extensive presentation of UnseenEst is now included. 

\closing{Sincerely,}

\end{letter}

\end{document}
